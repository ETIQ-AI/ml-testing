{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The etiqai library enables users to identify and mitigate bias in predictive models. The library is similar to pytorch or sklearn libraries, with a battery of metrics and debiasing algorithms available to find the best debiasing method for the  problem at hand. \n",
    "\n",
    "The DataPipeline object holds what we'd like to focus on during the debiasing process: the dataset used for training the model, the model we'd like to test and the fairness metrics used to evaluate results. DebiasPipeline objects take the initial DataPipeline object and apply to it different Identify methods, which aim to generate flags for rows at risk of being biased against, or Repair methods. Repair methods generate new \"debiased\" datasets. Models trained on debiased datasets perform better on fairness metrics. We have a (growing) collection of repair methods that allow our users to pick the debiased dataset version that best fits their criteria for a good solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a social enterprise, our mission is to address some of the most fundamental AI ethics issues to create a fairer world. We really appreciate your feedback on using this library, whether it's good or bad! With your support we firmly believe we can build a better world together. To access our full solution including hands on support, to submit any comments, feature requests or issues please login to our slack channel https://etiqcore.slack.com or email us at info@etiq.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The prediction problem setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate some of the library's features, we build a model that predicts whether an applicant makes over or under 50K using the Adult dataset https://archive.ics.uci.edu/ml/datasets/adult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for trying out the ETIQ.ai toolkit!\n",
      "\n",
      "Visit our getting started documentation at https://docs.etiq.ai/\n",
      "\n",
      "Visit our Slack channel at https://etiqcore.slack.com/ for support or feedback.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from etiq_core import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the library and see your results on the dashboard, you need to first make an account at https://dashboard.etiq.ai. In your account, select “Manage Access Tokens”\n",
    "Create a new access token and copy the result below in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 'Connection successful. Projects and pipelines will be displayed in the dashboard.')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etiq_login(\"https://dashboard.etiq.ai/\", \"LNP9yVOd4o5QSkO3yOLKuG2YJky98QTHNj9xGdLbkmPntGHGlPv110aPqHWerPh7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're loading the Adult dataset that is already available with the library, but users can use any dataset in a pandas dataframe format for their debiasing analysis. \n",
    "\n",
    "We're going to build a couple different pipelines to better understand the issues in this dataset. All the pipelines will be organzied under a project, visible in the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're starting \n",
    "\n",
    "our_project = Project(name=\"TestAdult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = load_sample('adultdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>198693</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29</td>\n",
       "      <td>?</td>\n",
       "      <td>227026</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>104626</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>3103</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>Private</td>\n",
       "      <td>369667</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>Private</td>\n",
       "      <td>104996</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt     education  educational-num  \\\n",
       "0   25           Private  226802          11th                7   \n",
       "1   38           Private   89814       HS-grad                9   \n",
       "2   28         Local-gov  336951    Assoc-acdm               12   \n",
       "3   44           Private  160323  Some-college               10   \n",
       "4   18                 ?  103497  Some-college               10   \n",
       "5   34           Private  198693          10th                6   \n",
       "6   29                 ?  227026       HS-grad                9   \n",
       "7   63  Self-emp-not-inc  104626   Prof-school               15   \n",
       "8   24           Private  369667  Some-college               10   \n",
       "9   55           Private  104996       7th-8th                4   \n",
       "\n",
       "       marital-status         occupation   relationship   race  gender  \\\n",
       "0       Never-married  Machine-op-inspct      Own-child  Black    Male   \n",
       "1  Married-civ-spouse    Farming-fishing        Husband  White    Male   \n",
       "2  Married-civ-spouse    Protective-serv        Husband  White    Male   \n",
       "3  Married-civ-spouse  Machine-op-inspct        Husband  Black    Male   \n",
       "4       Never-married                  ?      Own-child  White  Female   \n",
       "5       Never-married      Other-service  Not-in-family  White    Male   \n",
       "6       Never-married                  ?      Unmarried  Black    Male   \n",
       "7  Married-civ-spouse     Prof-specialty        Husband  White    Male   \n",
       "8       Never-married      Other-service      Unmarried  White  Female   \n",
       "9  Married-civ-spouse       Craft-repair        Husband  White    Male   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0             0             0              40  United-States  <=50K  \n",
       "1             0             0              50  United-States  <=50K  \n",
       "2             0             0              40  United-States   >50K  \n",
       "3          7688             0              40  United-States   >50K  \n",
       "4             0             0              30  United-States  <=50K  \n",
       "5             0             0              30  United-States  <=50K  \n",
       "6             0             0              40  United-States  <=50K  \n",
       "7          3103             0              32  United-States   >50K  \n",
       "8             0             0              40  United-States  <=50K  \n",
       "9             0             0              10  United-States  <=50K  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of columns: ['age' 'workclass' 'fnlwgt' 'education' 'educational-num' 'marital-status'\n",
      " 'occupation' 'relationship' 'race' 'gender' 'capital-gain' 'capital-loss'\n",
      " 'hours-per-week' 'native-country' 'income']\n",
      "Nr rows in the dataset: 48842\n"
     ]
    }
   ],
   "source": [
    "print('List of columns:', data.columns.values)\n",
    "print('Nr rows in the dataset:', data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard approach to training a binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict (classify) if a person's income is above or below 50K using this dataset. Following standard ML practice, after some data cleaning (removing rows with missing values and encoding categorical variables), we split the dataset into train/validate/test groups and train a model for this binary classification task, using the features in the dataset to predict the 'income' variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "# remove rows with missing values\n",
    "data = data.replace('?', np.nan)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# use a LabelEncoder to transform categorical variables \n",
    "cont_vars = ['age', 'educational-num', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "cat_vars = list(set(data.columns.values) - set(cont_vars))\n",
    "\n",
    "label_encoders = {}\n",
    "data_encoded = pd.DataFrame()\n",
    "for i in cat_vars:\n",
    "    label = LabelEncoder()\n",
    "    data_encoded[i] = label.fit_transform(data[i])\n",
    "    label_encoders[i] = label\n",
    "data_encoded.set_index(data.index, inplace=True)\n",
    "data_encoded = pd.concat([data.loc[:, cont_vars], data_encoded], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>income</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>native-country</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>relationship</th>\n",
       "      <th>education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>226802</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>89814</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>336951</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>160323</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>198693</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  educational-num  fnlwgt  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   25                7  226802             0             0              40   \n",
       "1   38                9   89814             0             0              50   \n",
       "2   28               12  336951             0             0              40   \n",
       "3   44               10  160323          7688             0              40   \n",
       "5   34                6  198693             0             0              30   \n",
       "\n",
       "   workclass  income  marital-status  occupation  native-country  race  \\\n",
       "0          2       0               4           6              38     2   \n",
       "1          2       0               2           4              38     4   \n",
       "2          1       1               2          10              38     4   \n",
       "3          2       1               2           6              38     2   \n",
       "5          2       0               4           7              38     4   \n",
       "\n",
       "   gender  relationship  education  \n",
       "0       1             3          1  \n",
       "1       1             0         11  \n",
       "2       1             0          7  \n",
       "3       1             0         15  \n",
       "5       1             1          0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the training/testing/validation datasets\n",
    "\n",
    "# separate into train/validate/test dataset of sizes 80%/10%/10% as percetages of the initial data\n",
    "data_remaining, test = train_test_split(data_encoded, test_size=0.1)\n",
    "train, valid = train_test_split(data_remaining, test_size=0.1112)\n",
    "\n",
    "# because we don't want to train on protected attributes or labels to be predicted, \n",
    "# let's remove these columns from the training dataset\n",
    "protected_train = train['gender'].copy() # gender is a protected attribute\n",
    "y_train = train['income'].copy() # labels we're going to train the model to predict\n",
    "x_train = train.drop(columns=['gender','income'])\n",
    "protected_valid = valid['gender'].copy() \n",
    "y_valid = valid['income'].copy() \n",
    "x_valid = valid.drop(columns=['gender','income'])\n",
    "protected_test = test['gender'].copy() \n",
    "y_test = test['income'].copy()\n",
    "x_test = test.drop(columns=['gender','income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a XGBoost model to predict 'income'\n",
    "\n",
    "standard_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=4)    \n",
    "model_fit = standard_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate is the model we trained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on the training dataset : 89.83 %\n",
      "Model accuracy on the validation dataset : 86.96 %\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = standard_model.predict(x_train)\n",
    "y_valid_pred = standard_model.predict(x_valid)\n",
    "print('Model accuracy on the training dataset :', \n",
    "      round(100 * accuracy_score(y_train, y_train_pred),2),'%') # round the score to 2 digits  \n",
    "\n",
    "print('Model accuracy on the validation dataset :', \n",
    "      round(100 * accuracy_score(y_valid, y_valid_pred),2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point in the analysis, we have a model that is reasonably accurate at predicting whether someone makes more or less than 50K given a few features for that person. But is our model fair?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The etiq library will help us:\n",
    "\n",
    "1. evaluate how fair is the model we just trained \n",
    "2. identify sources of bias \n",
    "3. repair the model, which means improve its performance on fairness metrics\n",
    "\n",
    "The fairness analysis is built around the model we'd like to evaluate, the dataset used to train it and the fairness metrics that are most relevant to our project, which are all wrapped in the DataPipeline object. \n",
    "\n",
    "Below, we define the parameters for the debiasing process. What is the protected category? Who is in the privileged and unprivileged groups? What is a positive outcome in this dataset? We include this information in the Dataset class. We define the model we're debiasing (the model we just built) and the fairness metrics we'll use to evaluate model fairness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the model that we want to evaluate for bias, the dataset used to train that model and the metrics we'd like to calculate into a DataPipeline object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance on fairness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To establish whether the model is fair, we check its performance on fairness metrics. The etiq package has implementations of fairness metrics that are commonly used in the algorithmic fairness literature. Some of the metrics available are : equal opportunity, equal odds, true negative rate, demographic parity. See our documentation for a full list. The user can also declare their own metric that might be more appropriate for their particular problem setup.\n",
    "\n",
    "We compute the metrics passed on when the DataPipeline object was initialized. Metrics are computed for the training and validation datset, using the model defined in the DataPipeline.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A commonly used fairness metric is Equal opportunity. The Equal opportunity metric calculates the True Positive Rate for each subgroup of a protected attribute like gender, age, race. A positive prediction from the model is an opportunity, such as \"this person makes over 50K, let's approve their mortgage application\". With the help of the etiqai library, we can check that the model is just as accurate in predicting who should get that opportunity regardless of gender, race, age. Something to remember is that the Equal opportunity metric accounts for different acceptance rates in the \"ground truth\" training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can test the performance of different models on the fairness metrics of interest. There are models preloaded in the library or users can use their own model. Below, we use a preloaded logistic regression model and ask for another fairness metric, demographic parity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the bias parameters: the protected category we'll look at is 'gender'. We label a negative outcome as being predicted to have a low income, and we define which are the privileged and unprivileged groups\n",
    "\n",
    "debias_param = BiasParams(protected='gender', privileged='Male', unprivileged='Female', \n",
    "                          positive_outcome_label='>50K', negative_outcome_label='<=50K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use etiq_core to apply the same data transformation as shown above in the standard aproach\n",
    "\n",
    "transforms = [Dropna, EncodeLabels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the DatasetLoader class to load the original Adult data into the Dataset class and apply transforms\n",
    "# the DatasetLoader accepts pandas or numpy data structures\n",
    "\n",
    "dl = DatasetLoader(data=data, label='income', transforms=transforms, bias_params=debias_param,\n",
    "                   train_valid_test_splits=[0.8, 0.1, 0.1], cat_col=cat_vars,\n",
    "                   cont_col=cont_vars, names_col = data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that we'd like to mitigate in tandem with the dataset.\n",
    "# users can load any model that has a predict function. Here we load a default architecture provided by the library.\n",
    "\n",
    "xgb = DefaultXGBoostClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which metrics we will use to evaluate bias\n",
    "# the library includes implementations of fairness metrics commonly used in the literature\n",
    "# users can implement their own metrics\n",
    "\n",
    "metrics_initial = [accuracy,  equal_opportunity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:etiq_core.pipeline.DataPipeline156:Etiq removed the column encoding the protected attribute values from the dataset. The models are fitted and metrics are computed on a dataset without the protected attribute column. The protected attribute values can be found in the protected_train or protected_valid fields of each dataset\n",
      "INFO:etiq_core.pipeline.DataPipeline156:Starting pipeline\n",
      "INFO:etiq_core.pipeline.DataPipeline156:Fitting model\n",
      "INFO:etiq_core.pipeline.DataPipeline156:Computed metrics for the initial dataset\n",
      "INFO:etiq_core.pipeline.DataPipeline156:Completed pipeline\n"
     ]
    }
   ],
   "source": [
    "# the initial pipeline computes metrics on the loaded dataset and model\n",
    "\n",
    "pipeline_initial = DataPipeline(dataset_loader=dl, model=xgb, metrics=metrics_initial, project=our_project)\n",
    "pipeline_initial.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DataPipeline156': [{'accuracy': ('privileged', 0.84, 'unprivileged', 0.93)},\n",
       "  {'equal_opportunity': ('privileged',\n",
       "    0.6901408450704225,\n",
       "    'unprivileged',\n",
       "    0.55)}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the metrics by group (privileged and unprivileged) for the model trained on the dataset\n",
    "\n",
    "pipeline_initial.get_protected_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite high accuracy for the unprivileged group, the xgb model scores lower on equal_opportunity. This results means that more people in the unprivileged group are mistakenly receiving negative outcome labels, when they should be receiving positive outcome labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:etiq_core.pipeline.DebiasPipeline911:Warning: potential data leak. You are evaluating a fitted model. If you use the same dataset that you used to train the model and you pass on just the training/validation/test split without passing on which was your validation dataset when you fitted the model, some observations that were previously in train can now be in test - which can skew your results\n",
      "INFO:etiq_core.pipeline.DebiasPipeline911:Starting pipeline\n",
      "INFO:etiq_core.pipeline.DebiasPipeline911:Start Phase IdentifyPipeline173\n",
      "INFO:etiq_core.pipeline.IdentifyPipeline173:Starting pipeline\n",
      "INFO:etiq_core.pipeline.IdentifyPipeline173:Completed pipeline\n",
      "INFO:etiq_core.pipeline.DebiasPipeline911:Completed Phase IdentifyPipeline173\n",
      "INFO:etiq_core.pipeline.DebiasPipeline911:Start Phase RepairPipeline500\n",
      "INFO:etiq_core.pipeline.RepairPipeline500:Starting pipeline\n",
      "INFO:etiq_core.pipeline.RepairPipeline500:Completed pipeline\n",
      "INFO:etiq_core.pipeline.DebiasPipeline911:Completed Phase RepairPipeline500\n",
      "INFO:etiq_core.pipeline.DebiasPipeline911:Refitting model\n",
      "INFO:etiq_core.pipeline.DebiasPipeline911:Computed metrics for the repaired dataset\n",
      "INFO:etiq_core.pipeline.DebiasPipeline911:Compare pipeline predictions\n",
      "INFO:etiq_core.pipeline.DebiasPipeline911:Completed pipeline\n"
     ]
    }
   ],
   "source": [
    "# the DebiasPipeline aims to identify sources of bias by applying analyses formalized in the Identify pipelines\n",
    "# the Identify pipeline is looking for 3 sources of bias (limited features, poor sampling and proxies)\n",
    "\n",
    "identify_pipeline = IdentifyBiasSources(nr_groups=20, # nr of segments based on using unsupervised learning to group similar rows\n",
    "                                        train_model_segment=True,\n",
    "                                        group_def=['unsupervised'],\n",
    "                                        fit_metrics=[accuracy, equal_opportunity])\n",
    "    \n",
    "# the DebiasPipeline aims to mitigate sources of bias by applying different types of repair algorithms\n",
    "# the library offers implementations of repair algorithms described in the academic fairness literature\n",
    "\n",
    "repair_pipeline = RepairResamplePipeline(steps=[ResampleUnbiasedSegmentsStep(ratio_resample=1)], random_seed=4)\n",
    "\n",
    "debias_pipeline = DebiasPipeline(data_pipeline=pipeline_initial, \n",
    "                                 model=xgb,\n",
    "                                 metrics=metrics_initial,\n",
    "                                 identify_pipeline=identify_pipeline,\n",
    "                                 repair_pipeline=repair_pipeline,\n",
    "                                 project=our_project)\n",
    "debias_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DebiasPipeline applied the repair algorithm we selected, using the information from the rows flagged by the Identify pipeline. The same metrics computed on the repaired dataset show that the gap in equal opportunity between the privileged and unprivileged group disappeared. To see the effect of the repair, compare equal_opportunity values for the initial pipeline (DataPipeline156) and the debias pipeline (DebiasPipeline911). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DataPipeline156': [{'accuracy': ('privileged', 0.84, 'unprivileged', 0.93)},\n",
       "  {'equal_opportunity': ('privileged',\n",
       "    0.6901408450704225,\n",
       "    'unprivileged',\n",
       "    0.55)}],\n",
       " 'DebiasPipeline911': [{'accuracy': ('privileged',\n",
       "    0.82,\n",
       "    'unprivileged',\n",
       "    0.91)},\n",
       "  {'equal_opportunity': ('privileged',\n",
       "    0.6539235412474849,\n",
       "    'unprivileged',\n",
       "    0.65)}]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debias_pipeline.get_protected_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What issues did we find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue</th>\n",
       "      <th>features</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>correlation_issue</td>\n",
       "      <td>age</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>limited_features_issue</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>low_unpriv_sample</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[0, 1, 2, 3, 5, 8, 9, 10, 13, 14, 15, 16, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>proxy_issue</td>\n",
       "      <td>relationship</td>\n",
       "      <td>[0, 1, 2, 3, 5, 8, 9, 10, 13, 14, 15, 16, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>skewed_unpriv_sample</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[1, 3, 5, 10, 14, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low_volume_group</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4, 6, 7, 11, 12, 17, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>missing_sample</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[11, 17]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    issue      features  \\\n",
       "0       correlation_issue           age   \n",
       "1  limited_features_issue           N/A   \n",
       "2       low_unpriv_sample           N/A   \n",
       "4             proxy_issue  relationship   \n",
       "5    skewed_unpriv_sample           N/A   \n",
       "0        low_volume_group           NaN   \n",
       "1          missing_sample           NaN   \n",
       "\n",
       "                                        segments  \n",
       "0                                            [0]  \n",
       "1                                           [14]  \n",
       "2  [0, 1, 2, 3, 5, 8, 9, 10, 13, 14, 15, 16, 19]  \n",
       "4  [0, 1, 2, 3, 5, 8, 9, 10, 13, 14, 15, 16, 19]  \n",
       "5                          [1, 3, 5, 10, 14, 15]  \n",
       "0                      [4, 6, 7, 11, 12, 17, 18]  \n",
       "1                                       [11, 17]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debias_pipeline.get_issues_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each segment, we can get a detailed view of the features that define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment</th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_segment_statistic</th>\n",
       "      <th>feature_segment_index</th>\n",
       "      <th>comparison_with_average</th>\n",
       "      <th>group_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>capital-gain</td>\n",
       "      <td>562.797055</td>\n",
       "      <td>50</td>\n",
       "      <td>below_average</td>\n",
       "      <td>2513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>239459.772384</td>\n",
       "      <td>126</td>\n",
       "      <td>above_average</td>\n",
       "      <td>2513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>capital-loss</td>\n",
       "      <td>69.253880</td>\n",
       "      <td>77</td>\n",
       "      <td>below_average</td>\n",
       "      <td>2513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>age</td>\n",
       "      <td>37.559491</td>\n",
       "      <td>97</td>\n",
       "      <td>within_avg_interval</td>\n",
       "      <td>2513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>educational-num</td>\n",
       "      <td>9.889375</td>\n",
       "      <td>97</td>\n",
       "      <td>within_avg_interval</td>\n",
       "      <td>2513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>19</td>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>269827.800488</td>\n",
       "      <td>142</td>\n",
       "      <td>above_average</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>19</td>\n",
       "      <td>capital-gain</td>\n",
       "      <td>837.608293</td>\n",
       "      <td>75</td>\n",
       "      <td>below_average</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>19</td>\n",
       "      <td>capital-loss</td>\n",
       "      <td>81.059024</td>\n",
       "      <td>91</td>\n",
       "      <td>within_avg_interval</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>19</td>\n",
       "      <td>age</td>\n",
       "      <td>37.504390</td>\n",
       "      <td>97</td>\n",
       "      <td>within_avg_interval</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>19</td>\n",
       "      <td>marital-status</td>\n",
       "      <td>2.664878</td>\n",
       "      <td>103</td>\n",
       "      <td>within_avg_interval</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     segment     feature_name  feature_segment_statistic  \\\n",
       "9          0     capital-gain                 562.797055   \n",
       "2          0           fnlwgt              239459.772384   \n",
       "10         0     capital-loss                  69.253880   \n",
       "0          0              age                  37.559491   \n",
       "4          0  educational-num                   9.889375   \n",
       "..       ...              ...                        ...   \n",
       "249       19           fnlwgt              269827.800488   \n",
       "256       19     capital-gain                 837.608293   \n",
       "257       19     capital-loss                  81.059024   \n",
       "247       19              age                  37.504390   \n",
       "252       19   marital-status                   2.664878   \n",
       "\n",
       "     feature_segment_index comparison_with_average  group_volume  \n",
       "9                       50           below_average          2513  \n",
       "2                      126           above_average          2513  \n",
       "10                      77           below_average          2513  \n",
       "0                       97     within_avg_interval          2513  \n",
       "4                       97     within_avg_interval          2513  \n",
       "..                     ...                     ...           ...  \n",
       "249                    142           above_average          2050  \n",
       "256                     75           below_average          2050  \n",
       "257                     91     within_avg_interval          2050  \n",
       "247                     97     within_avg_interval          2050  \n",
       "252                    103     within_avg_interval          2050  \n",
       "\n",
       "[65 rows x 6 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 5 features for each segment\n",
    "\n",
    "debias_pipeline.get_profiler_top5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural question is what issues remain after our initial attempt to repair the dataset? We build an Evaluate pipeline, whcih takes in a DebiasPipeline and again tries to identify issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:etiq_core.pipeline.EvaluateDebiasPipeline385:Starting pipeline\n",
      "WARNING:etiq_core.pipeline.DataPipeline18:Warning: potential data leak. You are evaluating a fitted model. If you use the same dataset that you used to train the model and you pass on just the training/validation/test split without passing on which was your validation dataset when you fitted the model, some observations that were previously in train can now be in test - which can skew your results\n",
      "WARNING:etiq_core.pipeline.DataPipeline18:Etiq removed the column encoding the protected attribute values from the dataset. The models are fitted and metrics are computed on a dataset without the protected attribute column. The protected attribute values can be found in the protected_train or protected_valid fields of each dataset\n",
      "INFO:etiq_core.pipeline.DataPipeline18:Starting pipeline\n",
      "INFO:etiq_core.pipeline.DataPipeline18:Refitting model\n",
      "INFO:etiq_core.pipeline.DataPipeline18:Computed metrics for the initial dataset\n",
      "INFO:etiq_core.pipeline.DataPipeline18:Completed pipeline\n",
      "WARNING:etiq_core.pipeline.DebiasPipeline959:Warning: potential data leak. You are evaluating a fitted model. If you use the same dataset that you used to train the model and you pass on just the training/validation/test split without passing on which was your validation dataset when you fitted the model, some observations that were previously in train can now be in test - which can skew your results\n",
      "INFO:etiq_core.pipeline.DebiasPipeline959:Starting pipeline\n",
      "INFO:etiq_core.pipeline.DebiasPipeline959:Start Phase IdentifyPipeline173\n",
      "INFO:etiq_core.pipeline.IdentifyPipeline173:Starting pipeline\n",
      "INFO:etiq_core.pipeline.IdentifyPipeline173:Completed pipeline\n",
      "INFO:etiq_core.pipeline.DebiasPipeline959:Completed Phase IdentifyPipeline173\n",
      "INFO:etiq_core.pipeline.DebiasPipeline959:Refitting model\n",
      "INFO:etiq_core.pipeline.DebiasPipeline959:Computed metrics for the initial dataset\n",
      "INFO:etiq_core.pipeline.DebiasPipeline959:Completed pipeline\n",
      "INFO:etiq_core.pipeline.EvaluateDebiasPipeline385:Completed pipeline\n"
     ]
    }
   ],
   "source": [
    "# run Identify methods on top of the repaired dataset\n",
    "    \n",
    "evaluate_debias = EvaluateDebiasPipeline(debias_pipeline=debias_pipeline,\n",
    "                                         identify_pipeline=identify_pipeline)\n",
    "evaluate_debias.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue</th>\n",
       "      <th>features</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>correlation_issue</td>\n",
       "      <td>age</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>limited_features_issue</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>low_unpriv_sample</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[0, 1, 2, 3, 5, 8, 9, 10, 13, 14, 15, 16, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>proxy_issue</td>\n",
       "      <td>relationship</td>\n",
       "      <td>[0, 1, 2, 3, 5, 8, 9, 10, 13, 14, 15, 16, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>skewed_unpriv_sample</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[1, 3, 5, 10, 14, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low_volume_group</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4, 6, 7, 11, 12, 17, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>missing_sample</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[11, 17]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    issue      features  \\\n",
       "0       correlation_issue           age   \n",
       "1  limited_features_issue           N/A   \n",
       "2       low_unpriv_sample           N/A   \n",
       "4             proxy_issue  relationship   \n",
       "5    skewed_unpriv_sample           N/A   \n",
       "0        low_volume_group           NaN   \n",
       "1          missing_sample           NaN   \n",
       "\n",
       "                                        segments  \n",
       "0                                            [0]  \n",
       "1                                           [14]  \n",
       "2  [0, 1, 2, 3, 5, 8, 9, 10, 13, 14, 15, 16, 19]  \n",
       "4  [0, 1, 2, 3, 5, 8, 9, 10, 13, 14, 15, 16, 19]  \n",
       "5                          [1, 3, 5, 10, 14, 15]  \n",
       "0                      [4, 6, 7, 11, 12, 17, 18]  \n",
       "1                                       [11, 17]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_debias.get_issues_summary_before_repair()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue</th>\n",
       "      <th>features</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>correlation_issue</td>\n",
       "      <td>capital-loss</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>correlation_issue</td>\n",
       "      <td>marital-status</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>proxy_issue</td>\n",
       "      <td>relationship</td>\n",
       "      <td>[0, 1, 3, 5, 6, 8, 13, 14, 16, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low_volume_group</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2, 4, 7, 9, 10, 11, 12, 15, 17, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>missing_sample</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[7, 9, 12, 17, 19]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               issue        features                              segments\n",
       "0  correlation_issue    capital-loss                                   [1]\n",
       "1  correlation_issue  marital-status                                  [14]\n",
       "3        proxy_issue    relationship    [0, 1, 3, 5, 6, 8, 13, 14, 16, 18]\n",
       "0   low_volume_group             NaN  [2, 4, 7, 9, 10, 11, 12, 15, 17, 19]\n",
       "1     missing_sample             NaN                    [7, 9, 12, 17, 19]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_debias.get_issues_summary_after_repair()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see that our analyses have been stored under one project. We can also quickly understand the KPIs (nr_issues tested, ny customers mislabeled due to bias). The Dashboard offers a much easier to see view of the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"nr_issues_found\": 70, \"nr_issues_tested\": 240, \"nr_pipelines_run\": 2, \"types_pipeline_run\": [\"data pipeline\", \"IdentifyBiasSources - groups by unsupervised labels\", \"debias pipeline\", \"Repair - Resample unbiased groups\"], \"data_pipeline_kpis\": [{\"pipeline_id\": 77, \"pipeline_name\": \"DataPipeline156\", \"nr_issues_tested\": 0, \"nr_issues_found\": 0, \"percentage_fn_privileged\": 30.985915492957748, \"percentage_fn_unprivileged\": 45.0, \"diff_priv_unpriv_due_mislabel\": 14.014084507042252, \"nr_customers_mislabeled_due_bias\": 11.351408450704225, \"additional_good_customers_potential_gain\": 6.306338028169014, \"decrease_in_diff_priv_unpriv_due_mislabel\": 0, \"decrease_in_nr_customers_mislabeled_due_bias\": 0, \"percentage_capture_additional_good_customers\": 0}, {\"pipeline_id\": 82, \"pipeline_name\": \"DataPipeline18\", \"nr_issues_tested\": 0, \"nr_issues_found\": 0, \"percentage_fn_privileged\": 34.60764587525151, \"percentage_fn_unprivileged\": 35.0, \"diff_priv_unpriv_due_mislabel\": 0.3923541247484934, \"nr_customers_mislabeled_due_bias\": 0.24718309859155085, \"additional_good_customers_potential_gain\": 0.1373239436619727, \"decrease_in_diff_priv_unpriv_due_mislabel\": 0, \"decrease_in_nr_customers_mislabeled_due_bias\": 0, \"percentage_capture_additional_good_customers\": 0}], \"debias_pipeline_kpis\": [{\"pipeline_id\": 80, \"pipeline_name\": \"DebiasPipeline911\", \"nr_issues_found\": 43, \"nr_issues_tested\": 120, \"percentage_fn_privileged\": 34.60764587525151, \"percentage_fn_unprivileged\": 35.0, \"diff_priv_unpriv_due_mislabel\": 0.3923541247484934, \"nr_customers_mislabeled_due_bias\": 0.24718309859155085, \"additional_good_customers_potential_gain\": 0.1373239436619727, \"decrease_in_diff_priv_unpriv_due_mislabel\": 97.20028715003588, \"decrease_in_nr_customers_mislabeled_due_bias\": 97.822445561139, \"percentage_capture_additional_good_customers\": 97.82244556113902}, {\"pipeline_id\": 83, \"pipeline_name\": \"DebiasPipeline959\", \"nr_issues_found\": 27, \"nr_issues_tested\": 120, \"percentage_fn_privileged\": 34.60764587525151, \"percentage_fn_unprivileged\": 35.0, \"diff_priv_unpriv_due_mislabel\": 0.3923541247484934, \"nr_customers_mislabeled_due_bias\": 0.24718309859155085, \"additional_good_customers_potential_gain\": 0.1373239436619727, \"decrease_in_diff_priv_unpriv_due_mislabel\": 0.0, \"decrease_in_nr_customers_mislabeled_due_bias\": 0.0, \"percentage_capture_additional_good_customers\": 0.0}], \"percentage_fn_privileged\": 30.985915492957748, \"percentage_fn_unprivileged\": 45.0, \"diff_priv_unpriv_due_mislabel\": 14.014084507042252, \"additional_good_customers_potential_gain\": 6.306338028169014, \"decrease_in_diff_priv_unpriv_due_mislabel\": 97.20028715003588, \"decrease_in_nr_customers_mislabeled_due_bias\": 97.822445561139, \"percentage_capture_additional_good_customers\": 97.82244556113902}'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_project.get_kpis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
